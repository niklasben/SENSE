<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="xsl/oai2.xsl"?><OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2014-10-15T06:58:41Z</responseDate>
  <request verb="GetRecord" metadataPrefix="oai_dc" identifier="oai:kobv.de-opus4-tuberlin:3112">http://opus4.kobv.de/opus4-tuberlin/oai</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:kobv.de-opus4-tuberlin:3112</identifier>
        <datestamp>2013-07-01</datestamp>
        <setSpec>doc-type:doctoralthesis</setSpec>
        <setSpec>bibliography:false</setSpec>
        <setSpec>ddc</setSpec>
        <setSpec>ddc:004</setSpec>
      </header>
      <metadata>
        <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
          <dc:title xml:lang="eng">Learning on Relational Data: Prototype-Based Classification of Attributed Graphs</dc:title>
          <dc:title xml:lang="deu">Lernen auf Relationalen Daten: Prototyp-basierte Klassifikation von Graphen mit Attributen</dc:title>
          <dc:creator>Srinivasan, Shankar Deepak</dc:creator>
          <dc:subject>ddc:004</dc:subject>
          <dc:description xml:lang="eng">In structural pattern recognition, attributed graphs are a very useful means to represent patterns. They enjoy many advantages over feature vectors, such as their ability to support relations between different components of a pattern which results in higher representative and descriptive power. However, in many potential applications, they are encumbered by a lack of repertoire of techniques to accomplish standard tasks such as classification and clustering. This is primarily because the ‚Äúspace of graphs‚Ä? has no rich mathematical structure like vector spaces. Even the most elementary concepts such as addition of graphs, mean of a set of graphs is not defined. A method has been proposed on working in the "space of structures", which associates all equivalent structures with vector representations through a membership function. In the case of graphs, elements of the same equivalence class would be vectors corresponding to all graphs obtained by all possible permutations of the nodes. Such an embedding of graphs enables the definition of metric and (sub)gradient, thus allowing the development of principled learning algorithms in the domain of relational data. Learning Graph Quantization (LGQ) is an algorithm for constructing a classifier that maps graphs to class labels from a finite set. The classifiers are parametrized by a set of prototypes with class labels. The class label of a new graph is predicted by assigning it to the class label of the nearest prototype (NPC) according to the nearest neighbor rule. The goal of learning is to find a set of prototypes that best predicts the class labels of graphs from the underlying distribution. With the notion of metric and subgradient defined, a novel class of algorithms are proposed to learn the prototypes using a subgradient descent procedure analogous to Learning Vector Quantization in the domain of feature vectors. Prototype based methods in structural pattern recognition can also be used to embed the graphs into a vector space. Consider the dissimilarity of the graphs to each prototype as a feature in a (feature) vector space. In such a view, the prototypes generate a kernel, which in turn generate features for classification. We demonstrate that the prototypes learnt by the class of LGQ algorithms generate features that are well suited for classification. In experiments, they improve the state-of-the art performance on multiple datasets. We also evaluate the quality of prototypes for embedding, using bounds from statistical learning theory. In the third part, a class of probabilistic methods is proposed for classifying attributed graphs. Within the framework of attributed random graphs, we propose an online algorithm to estimate the parameters, using concepts from Information geometry. The resulting random graph is chosen as a prototype and a formula for defining likelihood is proposed with suitable independence assumptions. The graph set is embedded into a feature space (defined by log-likelihood), where classifiers are trained. It is demonstrated that likelihood is a feature for classification.</dc:description>
          <dc:description xml:lang="deu">In der strukturellen Mustererkennung haben sich Graphen mit Attributen als n√ºtzliche Methode zur Repr√§sentation von Mustern erwiesen. Sie verf√ºgen √ºber viele Vorteile gegen√ºber Vektorr√§umen, wie etwa der M√∂glichkeit, Relationen zwischen verschiedenen Komponenten eines Musters zu repr√§sentieren. Es fehlt jedoch ein Repertoire an Methoden f√ºr allt√§gliche Aufgaben wie Klassifikation oder Clusterverfahren. Der wesentliche Grund daf√ºr ist das Fehlen von erweiterten mathematischen Strukturen auf Graphenr√§umen, wie sie f√ºr Vektorr√§ume existieren. Selbst die elementarsten Konzepte, wie die Addition von Graphen und der Mittelwert von Graphen, sind nicht definiert. Bei einer Methode, die vorgeschlagen wurde, wird in Strukturr√§umen gearbeitet. Hier werden mit Hilfe einer Zugeh√∂rigkeitsfunktion alle √§quivalenten Strukturen mit Vektorrepr√§sentationen assoziiert. Im Fall von Graphen w√ºrden dabei Elemente der selben √Ñquivalenzklasse als Vektoren repr√§sentiert, welche zu allen Graphen korrespondieren w√ºrden, die man √ºber Permutationen ihrer Knoten erhalten kann. Eine solche Einbettung von Graphen erm√∂glicht die Definition von Metriken und (Sub-)Gradienten. Mit Hilfe dieser Definitionen k√∂nnen prinzipielle Lernalgorithmen f√ºr relationale Daten entwickelt werden. Learning Graph Quantization (LGQ) ist ein Algorithmus zur Konstruktion eines Klassifikators, der Graphen auf Klassenbezeichner einer endlichen Menge abbildet. Der Klassifikator ist parametrisiert durch eine Menge von Prototypen mit Klassenbezeichnern. Der Klassenbezeichner eines neuen Graphen wird ermittelt, indem er dem Klassenbezeichner des n√§chstgelegenen Prototypen mit Hilfe einer N√§chstgelegener Nachbar-Regel zugeordnet wird. Das Ziel des Lernens ist es, eine Menge von Prototypen zu finden, die die Klassenbezeichner der Graphen am besten aus der zugrundeliegenden Verteilung ermitteln. Mit den definierten Konzepten der Metrik und Subgradienten wird eine neue Klasse von Algorithmen eingef√ºhrt, um die Prototypen durch ein Subgradientenabstiegsverfahren zu ermitteln, wie es auch bei dem Learning Vector Quantization Verfahren auf Vektorr√§ume angewandt wird. Prototyp-basierte Methoden in der strukturellen Mustererkennung k√∂nnen ebenfalls verwendet werden, um Graphen in Vektorr√§ume einzubetten. Die Un√§hnlichkeit zwischen den Graphen und jedem Prototypen wird als Merkmal in einem (Merkmals-) Vektorraum verstanden. Bei dieser Sichtweise generieren die Prototypen einen Kern, der wiederum Merkmale zur Klassifizierung generiert. Es wird gezeigt, dass die durch die Klasse der LGQ Algorithmen ermittelten Prototypen Merkmale generieren, die sehr gut zur Klassifizierung geeignet sind. In Experimenten mit mehreren Datens√§tzen verbessern sie die Ergebnisse von Methoden, die dem aktuellen Stand der Technik entsprechen. Au√üerdem wird die Qualit√§t der Prototypen f√ºr die Einbettung evaluiert, indem Schranken aus der statistischen Lerntheorie angewendet werden. Im dritten Teil der Arbeit wird eine Klasse von probabilistischen Verfahren eingef√ºhrt, um Graphen mit Attributen zu klassifizieren. F√ºr Zufallsgraphen mit Attributen schlagen wir einen Online-Algorithmus zur Parametersch√§tzung vor, der auf Konzepten der Informationsgeometrie basiert. Der resultierende Zufallsgraph wird dabei als Prototyp ausgew√§hlt. Zudem wird eine Formel zur Definition der Likelihood eingef√ºhrt, die √ºber geeignete Unabh√§ngigkeitsannahmen verf√ºgt. Die Menge der Graphen wird in einen Merkmalsraum eingebettet (definiert √ºber die Log-Likelihood), in dem dann Klassifikatoren trainiert werden. Es wird demonstriert, dass die Likelihood ein geeignetes Merkmal zur Klassifizierung ist.</dc:description>
          <dc:date>2011-11-01</dc:date>
          <dc:type>doctoralthesis</dc:type>
          <dc:type>doc-type:doctoralthesis</dc:type>
          <dc:format>application/pdf</dc:format>
          <dc:identifier>http://opus4.kobv.de/opus4-tuberlin/frontdoor/index/index/docId/3112</dc:identifier>
          <dc:identifier>urn:nbn:de:kobv:83-opus-32974</dc:identifier>
          <dc:identifier>http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:kobv:83-opus-32974</dc:identifier>
          <dc:identifier>http://opus4.kobv.de/opus4-tuberlin/files/3112/srinivasan_deepak.pdf</dc:identifier>
          <dc:language>eng</dc:language>
          <dc:rights>Deutsches Urheberrecht mit Print on Demand (u.a. f√ºr Dissertationen empfohlen)</dc:rights>
        </oai_dc:dc>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
