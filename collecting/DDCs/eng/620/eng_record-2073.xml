<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="xsl/oai2.xsl"?><OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
  <responseDate>2014-10-15T06:53:49Z</responseDate>
  <request verb="GetRecord" metadataPrefix="oai_dc" identifier="oai:kobv.de-opus4-tuberlin:2073">http://opus4.kobv.de/opus4-tuberlin/oai</request>
  <GetRecord>
    <record>
      <header>
        <identifier>oai:kobv.de-opus4-tuberlin:2073</identifier>
        <datestamp>2013-07-01</datestamp>
        <setSpec>doc-type:doctoralthesis</setSpec>
        <setSpec>bibliography:false</setSpec>
        <setSpec>ddc</setSpec>
        <setSpec>ddc:620</setSpec>
      </header>
      <metadata>
        <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
          <dc:title xml:lang="eng">From Sparse Models to Timbre Learning: New Methods for Musical Source Separation</dc:title>
          <dc:title xml:lang="deu">Dünnbesetzte und gelernte Klangfarbenmodelle zur Quellentrennung in der Musik</dc:title>
          <dc:creator>Burred, Juan José</dc:creator>
          <dc:subject>ddc:620</dc:subject>
          <dc:description xml:lang="eng">The goal of source separation is to detect and extract the individual signals present in a mixture. Its application to sound signals and, in particular, to music signals, is of interest for content analysis and retrieval applications arising in the context of online music services. Other applications include unmixing and remixing for post-production, restoration of old recordings, object-based audio compression and upmixing to multichannel setups. This work addresses the task of source separation from monaural and stereophonic linear musical mixtures. In both cases, the problem is underdetermined, meaning that there are more sources to separate than channels in the observed mixture. This requires taking strong statistical assumptions and/or learning a priori information about the sources in order for a solution to be feasible. On the other hand, constraining the analysis to instrumental music signals allows exploiting specific cues such as spectral and temporal smoothness, note-based segmentation and timbre similarity for the detection and extraction of sound events. The statistical assumptions and, if present, the a priori information, are both captured by a given source model that can greatly vary in complexity and extent of application. The approach used here is to consider source models of increasing levels of complexity, and to study their implications on the separation algorithm. The starting point is sparsity-based separation, which makes the general assumption that the sources can be represented in a transformed domain with few high-energy coefficients. It will be shown that sparsity, and consequently separation, can both be improved by using nonuniform-resolution time-frequency representations. To that end, several types of frequency-warped filter banks will be used as signal front-ends in conjunction with an unsupervised stereo separation approach. As a next step, more sophisticated models based on sinusoidal modeling and statistical training will be considered in order to improve separation and to allow the consideration of the maximally underdetermined problem: separation from single-channel signals. An emphasis is given in this work to a detailed but compact approach to train models of the timbre of musical instruments. An important characteristic of the approach is that it aims at a close description of the temporal evolution of the spectral envelope. The proposed method uses a formant-preserving, dimension-reduced representation of the spectral envelope based on spectral interpolation and Principal Component Analysis. It then describes the timbre of a given instrument as a Gaussian Process that can be interpreted either as a prototype curve in a timbral space or as a time-frequency template in the spectral domain. A monaural separation method based on sinusoidal modeling and on the mentioned timbre modeling approach will be presented. It exploits common-fate and good-continuation cues to extract groups of sinusoidal tracks corresponding to the individual notes. Each group is compared to each one of the timbre templates on the database using a specially-designed measure of timbre similarity, followed by a Maximum Likelihood decision. Subsequently, overlapping and missing parts of the sinusoidal tracks are retrieved by interpolating the selected timbre template. The method is later extended to stereo mixtures by using a preliminary spatial-based blind separation stage, followed by a set of refinements performed by the above sinusoidal modeling and timbre matching methods and aiming at reducing interferences with the undesired sources. A notable characteristic of the proposed separation methods is that they do not assume harmonicity, and are thus not based on a previous multipitch estimation stage, nor on the input of detailed pitch-related information. Instead, grouping and separation relies solely on the dynamic behavior of the amplitudes of the partials. This also allows separating highly inharmonic sounds and extracting chords played by a single instrument as individual entities. The fact that the presented approaches are supervised and based on classification and similarity allows using them (or parts thereof) for other content analysis applications. In particular the use of the timbre models, and the timbre matching stages of the separation systems will be evaluated in the tasks of musical instrument classification and detection of instruments in polyphonic mixtures.</dc:description>
          <dc:description xml:lang="deu">Das Ziel der Quellentrennung ist die Erkennung und Extraktion der einzelnen Signale, die in einer Mischung vorhanden sind. Ihre Anwendung auf Audiosignale und im Besonderen auf Musiksignale ist von großem praktischen Interesse im Rahmen der inhaltsbasierten Analyse für neue Online-Musikdienste und Multimediaanwendungen. Quellentrennung findet auch Einsatz in Studio-Nachbearbeitung, Wiederherstellung alter Aufnahmen, objektbasierter Audiocodierung und beim Erstellen neuer Mischungen für mehrkanalige Systeme. Die vorliegende Dissertation befasst sich mit der Aufgabe, Quellen aus linearen Mono- und Stereomusikmischungen zu extrahieren. In beiden Fällen ist die Aufgabenstellung unterbestimmt, d.h., es gibt mehr Quellen zu trennen als Kanäle in der Mischung vorhanden sind. Dies verlangt starke statistische Annahmen, bzw. das A-priori-Erlernen von Information über die Quellen. Andererseits erlaubt die Anwendung auf Musiksignale, spezifische Eigenschaften auszunutzen, wie etwa spektrale und zeitliche Glattheit, notenbasierte Segmentierung und Ähnlichkeit der Klangfarbe, um die einzelnen Klangereignisse zu erkennen und zu trennen. Sowohl die statistischen Annahmen als auch das eventuelle Vorwissen werden von einem bestimmten Quellenmodell erfasst. Ein solches Modell kann stark in Komplexität und Anwendbarkeit variieren. Der verwendete methodische Ansatz bestand daraus, verschiedene Quellenmodelle wachsender Komplexität zu betrachten und ihre jeweiligen Auswirkungen auf die Trennungsalgorithmen zu studieren. Der Ausgangsspunkt ist die Trennung basierend auf dünnbesetzten (sparse) Signalen, in welchem Fall angenommen wird, dass die Quellen in einem bestimmten Bereich mit wenigen energiereichen Koeffizienten dargestellt werden können. Es wird gezeigt, dass sparsity, und folglich Trennung, durch die Verwendung von Zeit-Frequenz Darstellungen nicht-linearer Auflösung verbessert werden. Zu diesem Zweck werden verschiedene Arten von frequenzverzerrten Filterbänken als Front-End im Zusammenhang mit einer unüberwachten Stereo-Trennungsmethode ausgewertet. Als nächster Schritt werden komplexere Modelle, basierend auf sinusoidaler Modellierung und statistischem Lernen, in Betracht gezogen. Sie erlauben, die maximal unterbestimmte Situation zu behandeln, nämlich die Trennung aus einer monophonen Mischung. Ein besonderer Schwerpunkt wird auf das Lernen eines detaillierten, wenngleich kompakten Modells der Klangfarbe gelegt. Die vorgeschlagene Methode benutzt eine formantenerhaltende, dimensionsreduzierte Darstellung der spektralen Hüllkurve, die auf spektraler Interpolation und auf Hauptkomponentenanalyse beruht. Eine wichtige Eigenschaft des Ansatzes ist die detaillierte Beschreibung des zeitlichen Verlaufs der Hüllkurve. Das resultierende Modell beschreibt die Klangfarbe eines Instrumentes entweder in Form einer Prototypkurve im Klangfarbenraum oder als eine Zeit-Frequenz-Schablone. Im Anschluss wird ein Ansatz für monophone Trennung, die auf solchen Klangfarbenmodellen basiert, vorgestellt. Er gruppiert die Partialtöne anhand von gemeinsamen dynamischen Eigenschaften. Jede Gruppe wird mit den erlernten Zeit-Frequenz-Schablonen verglichen, unter Benutzung eines Maßes von Klangfarbenähnlichkeit, gefolgt von einer Maximum-Likelihood-Entscheidung. Die überlappenden und unvollständigen Anteile werden vom Modell mittels Interpolation gewonnen. Diese Methode wird anschließend für Stereomischungen erweitert. Dafür wird ein Modul für blinde Stereoquellentrennung als Vorverarbeitungsstufe eingesetzt, gefolgt von einer Reihe Verfeinerungen, die durch die erwähnten sinusoidalen Methoden realisiert werden. Eine besondere Eigenschaft der vorgestellten Trennungmethoden ist, dass keine Harmonizität angenommen wird. Die Trennung basiert also nicht auf einer vorhandenen Analyse der Grundfrequenzen in der Mischung und verlangt keine Eingabe von Information über die vorhandenen Tonhöhen. Stattdessen beruht die Gruppierung und Trennung der Partialtöne lediglich auf dem dynamischen Verhalten ihrer Amplituden. Dies erlaubt ebenfalls die Trennung disharmonischer Klänge und die einheitliche Extraktion von Akkorden. Die Tatsache, dass die vorgeschlagenen Methoden überwacht sind und dass sie auf Klassifizierung und Ähnlichkeitsmessungen basieren, erlaubt ihre Verwendung für andere inhaltsbasierte Anwendungen. Somit werden die entwickelten Klangfarbenmodelle in monophonen und polyphonen Klassifizierungsaufgaben ausgewertet.</dc:description>
          <dc:date>2009-03-06</dc:date>
          <dc:type>doctoralthesis</dc:type>
          <dc:type>doc-type:doctoralthesis</dc:type>
          <dc:format>application/zip</dc:format>
          <dc:format>application/pdf</dc:format>
          <dc:identifier>http://opus4.kobv.de/opus4-tuberlin/frontdoor/index/index/docId/2073</dc:identifier>
          <dc:identifier>urn:nbn:de:kobv:83-opus-21661</dc:identifier>
          <dc:identifier>http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:kobv:83-opus-21661</dc:identifier>
          <dc:identifier>http://opus4.kobv.de/opus4-tuberlin/files/2073/original_burred_juanjose_latex_source.zip</dc:identifier>
          <dc:identifier>http://opus4.kobv.de/opus4-tuberlin/files/2073/burred_juanjose.pdf</dc:identifier>
          <dc:language>eng</dc:language>
          <dc:rights>Deutsches Urheberrecht mit Print on Demand (u.a. für Dissertationen empfohlen)</dc:rights>
        </oai_dc:dc>
      </metadata>
    </record>
  </GetRecord>
</OAI-PMH>
